# 评测指标
- 量化和误差  
> 在不同量化方案下，模型的损失函数和误差以 RMSE 指标表示  
- 损失函数：  
$$\mathcal{L}_{\text {tot }}=\frac{\lambda_q}{|C|} \sum_{j \in C} \mathcal{L}_Q\left(\frac{Q_j}{m_j}, \frac{\hat{Q}_j}{m_j}\right)+\frac{\lambda_d}{|C|} \sum_{j \in C} \mathcal{L}_{\mathbf{D}}\left(\mathbf{D}_j, \hat{\mathbf{D}}_j\right)$$

当前相关数据： 

量化策略 FP16 INT3  
模型参数量 1.3B 6.7B  
PPL 值 14.62 10.86  
量化后 PPL 12.47 10.13  
平均量化误差 (∆) 2.8% 4.4%  
量化因子 s=1 s=2  
变化的 ∆比例 0% 21.2%  

表2. 在不同量化条件下，模型的性能统计。通过调整量化因子 s > 1，有效降低了困惑度（PPL），从 14.62 降至 10.13。随着量化因子的增加，平均量化误差亦随之提升，但当 s=2 时，获得了最佳的 PPL 表现，后续的增加将导致非显著通道的量化误差显著提升。

在量化过程中，我们希望优化以下目标：  
$$s^* = \arg \min s L(s), L(s) = \| Q(W \cdot s)(s^{-1} \cdot X) - WX \|$$ (3)  
此处，Q 表示量化函数，例如 INT3 量化法，W 为 FP16 格式下的原始参数，X 为小型校准集缓存的输入特征。s 为每个通道的量化因子；为了提高稳定性，我们定义一个搜索空间，通过分析影响量化因子的因素来选取最优的量化因子。

我们的方法并不依赖于回归或反向传播，因而对校准集的依赖最小，避免了过拟合（见图6）。这一点确保了在量化过程中所需的数据量较少，并能有效保留与校准集分布之外的知识。具体细节请参见第3.3节。# 基准方法
## 在LLM压缩领域的有效性与可扩展性
### 动态指标:
- 权重分配比例(WSR): 描述不同权重在推理过程中的重要性分布。
- 量化损失函数(QLF): 衡量模型在低位量化过程中信息损失的度量标准。 WSR和QLF是在大规模语言模型(LLM)训练与推理场景中得出的，这些指标用来评估模型在面对复杂语境时的适应能力与有效性。

![img](https://example.com/image)

当前相关文本：

在这个研究中，我们提出了一种基于权重重要性的自适应量化方法，称为AWQ，该方法通过主观量化来降低显著权重的量化损失。与以往方法相比，AWQ对校准集的依赖更小，并且能在多样化的任务中保持LLMs的通用能力。我们的实验表明，AWQ在语言建模上表现优越，尤其在使用较小的校准集时，相较于GPTQ ， AWQ能显著提高准确性。所有实验均在OPT-6.7B模型下进行，并采用INT3-g128量化策略进行评估。

进一步而言，AWQ 在低比特重量量化的LLMs方面展示了很高的通用性和适应性。我们的实现方式将AWQ理论上的内存节省与实际测量加速相结合，实现了对FP16实现的3.2-3.3倍速度提升，这一过程不仅提升了移动与桌面GPU上的计算效率，同时也为边缘计算环境下LLM的广泛应用奠定了基础。

感谢MIT AI硬件项目、国家科学基金、NVIDIA学术合作奖、MIT-IBM沃森AI实验室、亚马逊与MIT科学中心、Qualcomm创新奖学金、微软图灵学术项目对本研究的支持。

参考文献
[1] Jean-Baptiste Alayrac, et al. Flamingo: 一种视觉语言模型用于少样本学习. 神经信息处理系统进展, 35:23716–23736, 2022.
[2] Anas Awadalla, 等. Openflamingo, 2023.# W4-RTN与W4-AWQ模型性能对比
## 量化模型在COCO数据集上的效果
该部分深入探讨了两种量化方法在COCO图像标注任务中的表现以及最终结果的定量分析。  
- W4-RTN与W4-AWQ的性能对比（视觉推理的精度）: 在图3中，研究者展示了量化的OpenFlamingo-9B模型在COCO数据集上的定性结果。量化方法显著提升了标注质量，相比之下，RTN基线的效果则明显不佳。为了对比标注质量，我们将文本颜色标记为正确或错误的标注。通过分析结果，W4-AWQ在视觉推理任务上表现出更好的能力，满足了多样化场景下的需求。

- 对不同模型的速度与效率分析: 图5展示了AWQ系统在多个设备上的加速评估结果。我们针对RTX 4090（桌面GPU）、RTX 4070（笔记本GPU）和Jetson Orin（移动GPU）进行基准测试，以批量大小为1进行推理。结果显示，AWQ方法为三类大型语言模型（Llama-2、MPT和Falcon）在4090上的处理速度提升了2.7-3.9倍。值得注意的是，在内存仅有8GB的笔记本4070上，我们依然可以以每秒33个标记的速度运行Llama-2-13B模型，而FP16实现则无法容纳7B模型。

此外，我们的方法在NVIDIA Jetson Orin（32GB）上也展现出良好性能。在图5(b)中，运行Llama-2模型时，AWQ系统达到了每秒33个标记的交互处理速率。得益于AWQ的实现，即使在资源有限的边缘设备上，更大模型如MPT-30B也可以顺畅运行，带来7.8个标记每秒的处理速度。值得一提的是，我们为所有AWQ模型使用本地PyTorch API实现前向推理，因此这段代码可在多种GPU架构中重用，为系统提供了最佳的推理速度和卓越的可扩展性。# 基准测试方法
在本研究中，我们使用了以下几种量化方法对LLaMA和Llama-2模型进行了评估：  
1. FP16 (16位浮点数)
2. INT3 (3位整数量化)
3. INT4 (4位整数量化)
4. AWQ (自适应权重量化)
5. GPTQ (全局量化方法)
6. GPTQ-R (经过重排序的GPTQ)

在实验中，我们对比了不同量化策略对语言模型的困惑度 (PPL) 的影响。具体来说，我们分析了在不同模型规模（7B, 13B, 30B, 65B, 70B）下，这些量化方法在困惑度评估上的表现。从表4中可以看出，AWQ在不同模型规格和位精度下，始终表现优于传统的四舍五入量化 (RTN) 和GPTQ（有无重排序均如此），这表明AWQ在保持性能的同时，能够有效降低模型体积。

3 实验设置
量化。我们专注于权重独立分组量化。正如之前的研究所示，分组量化通常能够改善性能与模型大小之间的权衡。除非另有说明，我们在整个工作中使用了128的组大小。我们重点关注INT4/INT3量化，因为它们能够在很大程度上保留大语言模型的性能。 

模型。我们在LLaMA和OPT模型上进行了基准测试。虽然还有其他开放的LLM，如BLOOM，但其质量相对较低，因此未列入研究范围。此外，我们还基准测试了指令调优模型Vicuna以及视觉语言模型OpenFlamingo-9B和LLaVA-13B，以展示我们方法的广泛适用性。

评估。为了确保评估结果的可靠性，我们主要在语言建模任务（WikiText-2困惑度评估）上对量化模型进行了分析。根据以往文献，困惑度能够稳定反映大语言模型的性能。

基线。我们的主要基线是传统的四舍五入量化 (RTN)，在使用小组大小（如128）时表现相当强劲。我们还与当前最先进的方法GPTQ进行了比较，并特别关注其重排序版本（GPTQ-R）。此外，其他如ZeroQuant、AdaRound及BRECQ等技术虽然依赖反向传播更新量化权重，但在大模型规模上并不易于扩展，故未纳入本研究。 

3.2 结果评估
关于LLaMA模型的结果。我们将重点放在LLaMA模型（LLaMA和Llama-2）上，因为其相比于其他开源LLM表现优越，并成为很多流行开源模型的基础。我们在表4中评估了量化前后的困惑度。结果表明，AWQ在不同模型规模（7B-70B）和代际中，持续优于传统RTN和GPTQ（有无重排序均如此）。 

指令调优模型的量化。指令调优显著提高模型性能和可用性，已成为模型部署前的重要步骤。我们进一步评估了指令调优模型Vicuna的量化性能，比较了基于GPT-4评分的量化模型与FP16对应模型在80个样本问题上的响应。通过对比两种先后顺序的响应，我们进行了160次试验，以消除序列效应的影响。AWQ在此背景下始终表现出色。**激活感知权重量化：大语言模型压缩与加速的创新方法**  
Ji Lin1∗，Jiaming Tang1,2∗，Haotian Tang1，Shang Yang1，Xingyu Dang3，Chuang Gan1，Song Han1  
1麻省理工学院 2上海交通大学 3清华大学  
https://github.com/mit-han-lab/llm-awq  

**摘要**  
大型语言模型（LLMs）在各类任务上表现卓越，但其庞大的模型规模提升了服务器的硬件门槛（内存大小），并减缓了令牌生成速度（内存带宽）。本文提出了一种新颖的激活感知权重量化（AWQ）方法，这是一种适合硬件的低比特权重量化方案。我们的设计基于以下观察：权重的重要性并不均等：仅保护1%的显著权重即可显著降低量化误差。因此，我们提出通过观察激活而非权重来搜索保护显著权重的最佳每通道缩放。AWQ不依赖于任何反向传播或重构，因此可以很好地保持LLM在不同领域和模态上的泛化能力，而不会对校准集过拟合。AWQ在各类语言建模和领域特定基准上超越现有方法。得益于更好的泛化能力，它为指令调优的语言模型和首次实现多模态语言模型提供了出色的量化性能。此外，我们实现了一个高效灵活的推理框架，专门针对边缘设备上的LLM，提供了超过3倍的速度提升，相较于Huggingface的FP16实现在桌面和移动GPU上表现优异。这一进展使得在移动GPU（NVIDIA Jetson Orin 64GB）上部署70B Llama-2模型变得更加民主化。

**1 引言**  
基于变换器的大型语言模型（LLMs）已在多个基准测试中展示了卓越的性能。然而，大模型的规模引发了高昂的服务成本。例如，GPT-3拥有1750亿参数，使用FP16格式时其内存占用达350GB，而最新的H100 GPU仅有96GB的内存，更不用提边缘设备了。  
对LLM进行低比特权重量化可以节省内存，但却很难实现。量化感知训练（QAT）由于高昂的训练成本而不切实际，而后训练量化（PTQ）在低比特设置下往往伴随明显的准确率下降。最接近的工作是GPTQ，它使用二阶信息进行误差补偿。但在重构过程中，可能会对校准集过拟合，扭曲了在分布外领域学习的特征，这非常关键，因为LLM是通用模型。  
在本研究中，我们提出了激活感知权重量化（AWQ），这是一种针对LLM的硬件友好型低比特权重仅量化方法。我们的方法基于以上观察，即权重在LLMs表现中的重要性并不相同。实际上，只有极小的一部分（0.1%-1%）显著权重；跳过这些显著权重的量化将显著减少量化损失。为了寻找显著权重通道，我们认为应该参考激活分布而非权重分布，尽管我们进行的是权重仅量化：与更大激活幅度对应的权重通道更为显著，因为它们处理更多的输入。  
∗表示等同贡献。  
预印本。待审。arXiv:2306.00978v2 [cs.CL]  2023年10月3日# 性能提升方法
本研究主要探索了量化模型中的显著权重保护方法：  
- 显著权重保护方法:  
1. 在FP16中保持0.1%-1%的权重显著改善了量化模型的性能，特别是在使用基于激活分布选择重要权重时，而非权重分布。  
2. 使用INT3量化，组大小为128，测量WikiText的困惑度(↓)。  
3. 通过对输入特征进行分析，发现较大幅度的输入特征通常更为重要，而保持相应权重在FP16中可以保留这些特征，从而提高模型性能。  
4. 尽管在FP16中保持0.1%的权重可以提升量化性能，但这种混合精度数据类型会导致系统实现的复杂性，需要寻找不同的方法来保护重要权重，而不必实际以FP16保留。  

- 基于激活的缩放方法:  
1. 通过每通道缩放的方法来减少显著权重量化错误，而不受硬件效率问题的影响。  
2. 量化误差分析显示，权重的线性操作可以表示为y=wx，量化后的对应形式为y=Q(w)x，其中Q(w)定义为量化函数。  
3. 研究发现，单个元素w的缩放并不改变权重组的极值，因此可以保持相对较小的量化误差。  
4. 在OPT-6.7B模型中，通过将1%的显著通道乘以s >1进行缩放，观察到困惑度从23.54改善到了11.92。  

- 选取最佳缩放因子:  
1. 在考虑显著和非显著权重时，通过自动搜索最优缩放因子来最小化输出差异。  
2. 需要同时关注非显著通道的误差，以确保在保护显著通道的同时，不会损害模型的整体准确性。  
3. 通过记录不同s值的∂变化，验证了优化缩放的有效性，以保证模型性能在总体上得到提升。# Benchmark方法
## 预测语言模型性能的准确性和可靠性
### 静态指标:
- 语言模型规模: 决定了模型的潜在表达能力，影响其在各种任务中的泛化能力和推理质量。
- 数据集的多样性: 数据集的丰富程度直接影响训练效果，确保模型在多种上下文中的适应性。
- 训练轮次: 训练过程中轮次的选择影响模型对训练数据的拟合程度，从而关系到其在未见数据上的性能表现。  
![img](https://w0zqg6m28ev.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjA1MjM0YjYwNzMzMzFkNDE3ZmJlMTY0ZDk3YTY1Zjc4X0pUVEtudW1jTmdRSnhZc2x5a0lXQlJhY1pVNU9kQmdMUlRvbnBoc1ZjaFY1MlZ5VkdBa28zVUpHMWprVDR9)  
- 结果与讨论:
- 总体而言，对于LLM和Chatbot模型，GPT-4、Vicuna、和GPT-3.5在多个任务中都表现出色。
- 基于few-shot学习的模型在数据稀缺情况下展示了良好的性能，但在特定领域的适应性仍然有限。
- 有关模型对不同类型输入的处理能力，ChatGPT在开放域对话中表现优异，而结构化查询方面则显得不够灵活。
- 需要注意的是，尽管多轮对话中表现出显著的上下文保持能力，模型在非常长的上下文中往往发生信息丢失。
- 同样，模型对少量样本的学习能力存在显著的变异性，部分模型在特定任务上的性能不均衡。
- 例如，在时间序列预测任务中，Vicuna显著超越其他对手，但在图像描述生成任务上则遭遇挑战。 

### 未来方向:
- 继续优化针对特定任务的数据集，使模型能力更加全面和均衡。
- 深入研究如何提升长期上下文记忆的能力，以增强输出的连贯性和一致性。
- 探索不同量化方法对推理速度和性能的影响，以实现大规模模型在资源受限环境下的应用。# 量化方法
## 模型性能评估的准确性和可靠性
这一部分主要探讨了一套用于评估模型量化方法在处理大型语言模型（LLMs）时的性能指标，并讨论了基于这些指标的不同方法比较结果。

表7. 我们的方法在量化过程中展现出独特优势，尤其是在低比特量化（INT2- g64）情况下，与GPTQ结合使用时，进一步缩小了性能差距。结果为OPT模型在WikiText-2数据集上的困惑度表现。
050100150200
130
48
165
105
210
50
64
64
31
62
55Huggingface (FP16)AWQ (FP16)AWQ (W4A16)每秒处理的Tokens FP16 OOM(a) RTX 4090桌面GPU(b) Jetson Orin移动GPUFP16 OOMLlama-2 (7B)Llama-2 (13B)MPT (7B)MPT (30B)Falcon (7B)010203040
24
20
34
19
36
10
13
13
8
12
12FP16 OOMFP16 OOMLlama-2 (7B)Llama-2 (13B)MPT (7B)MPT (30B)Falcon (7B)015304560
54
62
35
62
Llama-2 (7B)Llama-2 (13B)MPT (7B)Falcon (7B)(c) RTX 4070笔记本GPU
图6. AWQ提供了一种便捷的解决方案，将理论上的内存占用减少转化为可量化的加速效果。因此，AWQ在4090（桌面GPU）和Orin（移动GPU）上分别比Huggingface的FP16实现快3.9倍和3.5倍。AWQ还使得在仅有8GB内存的笔记本GPU（4070）上，Llama-2-13B的部署变得可行。

3.4 数据效率分析
更好的校准集数据效率。我们的方法对校准集的需求较小，因为我们不依赖于回归或者反向传播；我们仅测量来自校准集的平均激活幅度，这种方法更具数据效率。为验证该点，我们在图7（a）中比较了OPT-6.7B模型在INT3-g128量化下的困惑度。AWQ在达到良好的量化性能方面，所需的校准集也显著较小；与GPTQ相比，AWQ使用的校准集规模小了10倍（16个序列对比192个序列）。

对校准集分布的鲁棒性。我们的方法对校准集的分布敏感性较低，因为我们仅测量来自校准集的平均激活幅度，这使得该方法在不同数据集分布中更具普适性。我们进一步在图7(b)中基准测试了不同校准集分布的影响。我们从Pile数据集中提取了两个子集：PubMed摘要和Enron邮件。我们使用每个子集作为校准集，并在两个集合上评估量化模型（校准和评估集不重叠；我们使用1000个样本进行评估）。总体而言，使用相同校准和评估分布时效果最佳（PubMed-PubMed，Enron-Enron）。然而，当使用不同的校准分布时（PubMed-Enron，Enron-PubMed），AWQ仅使困惑度增加了0.5-0.6，而GPTQ则增幅达2.3-4.9。这表明AWQ对校准集分布的鲁棒性。

4 相关工作
模型量化方法。量化减少了深度学习模型的比特精度，这有助于缩小模型规模并加速推理。量化技术通常分为两类：量化感知训练（QAT，依赖于回传更新量化权重）和训练后量化（PTQ，通常是无训练的）。QAT方法在大模型如LLMs上无法轻易扩展，因此人们通常使用PTQ方法对LLMs进行量化。

LLM的量化研究。学者们研究了LLM量化的两种设置：（1）W8A8量化，其中激活和权重均量化为INT8；（2）低比特权重仅量化（例如，W4A16），仅将权重量化为低比特整数。我们在本研究中聚焦于第二种设置，因为它不仅降低了硬件门槛（需要更小的内存），而且加速了标记生成（缓解了内存受限的工作负载）。除了传统的四舍五入基线（RTN），GPTQ是与我们的工作最为接近的方法。然而，GPTQ的重构过程导致了对校准集的过拟合问题。# 量化方法
## Vicuna模型在多模态任务中的表现对比
![img](https://w0zqg6m28ev.feishu.cn/space/api/box/stream/download/asynccode/?code=OTU0YjEwODRlNmVjN2U4ZjYyMjBkZmMwOWVjMDU1MzdfR2l6dUhmQUlOOVQ0TEttdWtVeUhiWXBiZzZKODJEdHBfVG9rZW46RHRDcmI4QkJkb2J0cXZ4a2VyZmNiODdjblJnXzE3MjM2MjYzNDE6MTcyMzYyOTk0MV9WNA)  
本文探讨了Vicuna-7B和Vicuna-13B模型在量化过程中针对不同量化方法（GPTQ、RTN和AWQ）在多模态任务中的表现。  
1. 模型对比: 在COCO数据集的多个量化设置下, AWQ相较于RTN和GPTQ表现出更好的性能。结果显示，AWQ在零样本和少样本的设置下显著减少了量化性能的退化，表明其对多模态和上下文学习负载的良好适应性。
2. 实验结果: 在COCO标题生成任务中，AWQ在量化性能上超越了现有方法，即使在极低比特（INT2）量化下，相较于RTN仍保持了显著的困惑度改善。此外，AWQ的不依赖于校准集的特性，使其能够有效应用于视觉语言模型，从而在强度较低的量化设置中依然能够保证合理的效果。

通过对多个样本的平均性能测评，本研究的结果显示AWQ在32-shot条件下，量化退化从4.57减少至1.17，相比之下，FP16模型的性能稍显优势，但AWQ在模型体积上的压缩提供了更为实用的方案。此项研究对视觉语言模型(LVM)的低比特量化提供了前所未有的见解。# **论文主要内容**
1. 提出了基于激活感知的权重量化方法(AWQ)，该方法通过保护重要权重来提高LLM量化模型的准确性，无需训练或回归。这种方法为大型语言模型(LLMs)的量化提供了新的思路，有助于减轻量化带来的性能下降。
2. 通过对激活分布的分析，识别出1%的显著权重，从而实现对这些重要权重的量化保护。这一策略经过实验证明，可以显著提升量化后的模型性能，降低因量化损失导致的性能衰减。
3. 开发了一种数据驱动的方法用于搜索最优缩放参数，以减少量化误差。该方法能够在全权重量化的情况下，自动寻找最小化量化误差的缩放方案，增强了模型在不同领域和模态的泛化能力。
4. AWQ的实现通过内核融合技术来优化系统，显著减少了推理开销，将量化的理论内存节省转化为实际的加速效果。实验结果表明，AWQ在多个任务上超越了现有的方法，并展现了良好的量化性能，尤其是在指令调优的语言模型和多模态模型上的首次应用。
5. 实验表明，相较于Huggingface的FP16实现，AWQ在多种大型语言模型中一致性地实现了3.2-3.3倍的平均加速。这一成果为在资源有限的设备上进行高效推理提供了可能性，推动了LLMs的更广泛应用。